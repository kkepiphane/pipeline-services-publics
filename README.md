# ğŸ‡¹ğŸ‡¬ Pipeline Big Data - Services Publics du Togo

Pipeline de donnÃ©es automatisÃ© pour l'ingestion, le traitement et l'analyse des demandes de services publics au Togo.

**Version**: 2.0.0  
**Date**: DÃ©cembre 2025  
**Technologies**: Apache Airflow, Apache Spark, MongoDB, PostgreSQL, Docker

---

## ğŸ“‹ Table des matiÃ¨res

1. [Vue d'ensemble](#-vue-densemble)
2. [Architecture](#-architecture)
3. [PrÃ©requis](#-prÃ©requis)
4. [Installation rapide](#-installation-rapide-3-minutes)
5. [Installation dÃ©taillÃ©e](#-installation-dÃ©taillÃ©e)
6. [Utilisation du pipeline](#-utilisation-du-pipeline)
7. [VÃ©rification des donnÃ©es](#-vÃ©rification-des-donnÃ©es)
8. [RequÃªtes SQL utiles](#-requÃªtes-sql-utiles)
9. [Commandes de gestion](#-commandes-de-gestion)
10. [DÃ©pannage](#-dÃ©pannage)
11. [Structure du projet](#-structure-du-projet)
12. [Flux de donnÃ©es dÃ©taillÃ©](#-flux-de-donnÃ©es-dÃ©taillÃ©)
13. [Configuration avancÃ©e](#-configuration-avancÃ©e)
14. [Limites et amÃ©liorations](#-limites-et-amÃ©liorations)

---

## ğŸ¯ Vue d'ensemble

Ce pipeline permet de :
- âœ… **IngÃ©rer** des donnÃ©es hÃ©tÃ©rogÃ¨nes depuis MongoDB
- âœ… **Harmoniser** 3 structures de donnÃ©es diffÃ©rentes en un schÃ©ma unique
- âœ… **Nettoyer** et enrichir les donnÃ©es (dates, coordonnÃ©es GPS, statuts)
- âœ… **AgrÃ©ger** des statistiques par commune, type et pÃ©riode
- âœ… **Stocker** dans PostgreSQL pour l'analyse
- âœ… **Automatiser** l'exÃ©cution quotidienne via Airflow

### Cas d'usage
- Analyse des demandes de services publics (Ã©clairage, voirie, assainissement)
- Tableaux de bord pour la prise de dÃ©cision
- Suivi des taux de rÃ©solution par commune
- Identification des zones Ã  forte demande

---

## ğŸ—ï¸ Architecture

### SchÃ©ma global

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Fichier JSON  â”‚  
â”‚  (8829 demandes)â”‚  
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ Script Python
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    MongoDB      â”‚ --> â”‚   Apache Spark   â”‚ --> â”‚   Data Lake      â”‚
â”‚  (Source NoSQL) â”‚     â”‚   (Ingestion)    â”‚     â”‚  (Parquet/RAW)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                            â”‚
                                                            â†“
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚  Apache Spark    â”‚ --> â”‚   PostgreSQL     â”‚
                        â”‚  (Processing)    â”‚     â”‚  (Data Warehouse)â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                           â”‚
                                                           â†“
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚    Tables Analytiques                      â”‚
                        â”‚  â€¢ demandes_cleaned (8829 lignes)         â”‚
                        â”‚  â€¢ stats_type_localisation                â”‚
                        â”‚  â€¢ stats_temporelles                      â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

                    ğŸ”„ Orchestration : Apache Airflow
                    ğŸ“Š Monitoring : Interface Web Airflow
```

### Composants du systÃ¨me

| Composant | Image Docker | Port | RÃ´le |
|-----------|-------------|------|------|
| **MongoDB** | mongo:7.0 | 27017 | Base de donnÃ©es source (NoSQL) |
| **PostgreSQL** | postgres:15 | 5432 | Data Warehouse (analytics) |
| **Airflow Webserver** | Custom (Spark+Airflow) | 8080 | Interface d'orchestration |
| **Airflow Scheduler** | Custom (Spark+Airflow) | - | ExÃ©cution des tÃ¢ches |
| **Spark** | IntÃ©grÃ© dans Airflow | - | Traitement distribuÃ© (mode local) |

### Architecture technique

**Mode de dÃ©ploiement** : Docker Compose avec 5 conteneurs
- âœ… Mode Spark **local[*]** (simplifiÃ©, sans cluster standalone)
- âœ… Tous les services sur le mÃªme rÃ©seau Docker
- âœ… Volumes persistants pour MongoDB et PostgreSQL
- âœ… Connexions sÃ©curisÃ©es entre services

---

## ğŸ“¦ PrÃ©requis

### Obligatoire

- **Docker Desktop** >= 20.10
  - [TÃ©lÃ©charger pour Windows](https://www.docker.com/products/docker-desktop)
  - [TÃ©lÃ©charger pour Mac](https://www.docker.com/products/docker-desktop)
  - [TÃ©lÃ©charger pour Linux](https://docs.docker.com/desktop/install/linux-install/)
  
- **Docker Compose** >= 2.0 (inclus dans Docker Desktop)

- **Minimum 8 Go de RAM** disponible pour Docker
  - Configuration : Docker Desktop â†’ Settings â†’ Resources â†’ Memory

- **10 Go d'espace disque** libre

### Optionnel (pour dÃ©veloppement)

- Python 3.10+ avec pip
- Git
- Un client SQL (DBeaver, pgAdmin) ou un IDE avec support PostgreSQL

### VÃ©rification des prÃ©requis

```bash
# VÃ©rifier Docker
docker --version
# Attendu: Docker version 20.10.x ou plus

# VÃ©rifier Docker Compose
docker-compose --version
# Attendu: Docker Compose version 2.x.x

# VÃ©rifier que Docker fonctionne
docker ps
# Doit afficher les en-tÃªtes de colonnes (peut Ãªtre vide)

# VÃ©rifier la RAM allouÃ©e
docker system info | grep "Total Memory"
# Doit afficher au moins 8 Go
```

---

## ğŸš€ Installation rapide (3 minutes)

### Ã‰tape 1 : TÃ©lÃ©charger le projet

```bash
# Option A : Avec Git
git clone <URL_DU_REPO>
cd pipeline-services-publics

# Option B : TÃ©lÃ©charger le ZIP
# DÃ©compresser et ouvrir le terminal dans le dossier
```

### Ã‰tape 2 : Placer les donnÃ©es

```bash
# Copier votre fichier JSON dans le dossier data/
cp /chemin/vers/demandes_services_publics_togo.json data/

# VÃ©rifier que le fichier est bien lÃ 
ls -lh data/demandes_services_publics_togo.json
```

**â±ï¸ DurÃ©e : 3-5 minutes**

Le script va automatiquement :
1. âœ“ VÃ©rifier Docker et Docker Compose
2. âœ“ Nettoyer les anciens conteneurs
3. âœ“ CrÃ©er la structure de dossiers
4. âœ“ TÃ©lÃ©charger le driver PostgreSQL (si absent)
5. âœ“ Construire les images Docker
6. âœ“ DÃ©marrer PostgreSQL et MongoDB
7. âœ“ Initialiser Airflow (base de donnÃ©es + utilisateur)
8. âœ“ DÃ©marrer Airflow (webserver + scheduler)
9. âœ“ Charger les donnÃ©es dans MongoDB

### Ã‰tape 3 : AccÃ©der Ã  l'interface

Ouvrez votre navigateur : **http://localhost:8080**

- **Username** : `admin`
- **Password** : `admin`

---

## ğŸ”§ Installation dÃ©taillÃ©e

### 1. PrÃ©parer l'environnement

```bash
# CrÃ©er le dossier du projet
mkdir pipeline-services-publics
cd pipeline-services-publics

# CrÃ©er la structure des sous-dossiers
mkdir -p dags logs plugins spark_jobs scripts jars data/{raw,processed} init-mongo init-postgres
```

### 2. docker-compose.yml


#### Dockerfile


#### requirements.txt

### 3. TÃ©lÃ©charger le driver PostgreSQL

```bash
# CrÃ©er le dossier jars
mkdir -p jars

# TÃ©lÃ©charger le driver JDBC PostgreSQL
curl -L -o jars/postgresql-42.6.0.jar \
  https://jdbc.postgresql.org/download/postgresql-42.6.0.jar
```

### 4. Placer vos fichiers

Copiez tous les fichiers fournis (DAG, scripts Spark, scripts d'initialisation) dans leurs dossiers respectifs :

```
pipeline-services-publics/
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ pipeline_services_publics.py
â”œâ”€â”€ spark_jobs/
â”‚   â”œâ”€â”€ ingestion.py
â”‚   â””â”€â”€ processing.py
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ load_data_to_mongo.py
â”œâ”€â”€ init-mongo/
â”‚   â””â”€â”€ init.js
â”œâ”€â”€ init-postgres/
â”‚   â””â”€â”€ 01_create_db.sql
â””â”€â”€ data/
    â””â”€â”€ demandes_services_publics_togo.json
```

### 5. DÃ©marrer les services

```bash
# Construire les images
docker-compose build

# DÃ©marrer tous les services
docker-compose up -d

# VÃ©rifier que tout est dÃ©marrÃ©
docker-compose ps
```

## ğŸ“Š Utilisation du pipeline

### AccÃ©der Ã  Airflow

1. Ouvrir **http://localhost:8080** dans votre navigateur
2. Se connecter :
   - **Username** : `admin`
   - **Password** : `admin`

### Activer le DAG

1. Dans la liste des DAGs, trouver `pipeline_services_publics_togo`
2. Cliquer sur le **toggle** (interrupteur) pour l'activer
3. Le DAG est maintenant programmÃ© pour s'exÃ©cuter tous les jours Ã  2h00

### DÃ©clencher manuellement

#### Depuis l'interface Web

1. Cliquer sur le DAG `pipeline_services_publics_togo`
2. Cliquer sur le bouton **"Play"** (â–¶ï¸) en haut Ã  droite
3. SÃ©lectionner **"Trigger DAG"**
4. Confirmer

#### Depuis la ligne de commande

```bash
# MÃ©thode 1 : Via Make (recommandÃ©)
make trigger-dag

# MÃ©thode 2 : Commande directe
docker exec -it $(docker ps -q -f name=airflow-scheduler) \
  airflow dags trigger pipeline_services_publics_togo
```

### Suivre l'exÃ©cution

Dans l'interface Airflow :

1. **Vue "Graph"** : Visualise les dÃ©pendances entre tÃ¢ches
2. **Vue "Tree"** : Affiche l'historique des exÃ©cutions
3. **Vue "Gantt"** : Montre la durÃ©e de chaque tÃ¢che
4. **Logs** : Cliquer sur une tÃ¢che â†’ "Logs" pour voir les dÃ©tails

### Ã‰tapes du pipeline (7 tÃ¢ches)

| Ordre | TÃ¢che | DurÃ©e | Description |
|-------|-------|-------|-------------|
| 1 | `load_data_to_mongo` | 10-20s | Charge le JSON dans MongoDB |
| 2 | `check_mongodb` | 5s | VÃ©rifie la connexion et compte les documents |
| 3 | `create_directories` | 2s | CrÃ©e les dossiers de travail |
| 4 | `ingestion_mongodb` | 1-2min | Extrait depuis MongoDB â†’ Parquet (RAW) |
| 5 | `processing_spark` | 2-3min | Nettoie, harmonise, agrÃ¨ge â†’ PostgreSQL |
| 6 | `check_output` | 3s | Valide la prÃ©sence des fichiers Parquet |
| 7 | `generate_report` | 1s | Affiche un rapport de succÃ¨s |

**DurÃ©e totale : 4-6 minutes**

### Ã‰tat des tÃ¢ches (codes couleur)

- ğŸŸ¢ **Vert (success)** : TÃ¢che rÃ©ussie
- ğŸ”´ **Rouge (failed)** : TÃ¢che en Ã©chec (voir les logs)
- ğŸŸ¡ **Jaune (running)** : TÃ¢che en cours
- âšª **Gris (queued)** : TÃ¢che en attente
- ğŸ”µ **Bleu clair (upstream_failed)** : Ã‰chec d'une tÃ¢che prÃ©cÃ©dente

---
## âœ… VÃ©rification des donnÃ©es

### MÃ©thode 1 : Script de vÃ©rification automatique

```bash
# ExÃ©cuter le script de vÃ©rification
chmod +x check_status.sh
./check_status.sh

# Ou avec Make
make status
```

**Sortie attendue :**
```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  VÃ‰RIFICATION DU PIPELINE
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â„¹ 1. Ã‰tat des conteneurs Docker
mongodb  Up 5 minutes
postgres Up 5 minutes
airflow-webserver Up 5 minutes
airflow-scheduler Up 5 minutes

âœ“ Services Docker opÃ©rationnels

â„¹ 2. VÃ©rification MongoDB
âœ“ MongoDB : 8829 documents

â„¹ 3. VÃ©rification PostgreSQL
âœ“ PostgreSQL : 8829 lignes dans demandes_cleaned

â„¹ 4. VÃ©rification des fichiers Parquet
âœ“ Fichiers Parquet : 5 fichiers trouvÃ©s

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  RÃ‰SUMÃ‰
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“Š Statistiques :
   â€¢ MongoDB    : 8829 documents
   â€¢ PostgreSQL : 8829 lignes
   â€¢ Parquet    : 5 fichiers

âœ“ Pipeline opÃ©rationnel ! ğŸ‰
```

### MÃ©thode 2 : VÃ©rifier PostgreSQL

```bash
# Compter les enregistrements
docker exec -it $(docker ps -q -f name=postgres) \
  psql -U airflow -d airflow -c \
  "SELECT COUNT(*) FROM demandes_cleaned;"

# Voir un Ã©chantillon
docker exec -it $(docker ps -q -f name=postgres) \
  psql -U airflow -d airflow -c \
  "SELECT * FROM demandes_cleaned LIMIT 5;"

# Ou avec Make
make check-pg
```

### MÃ©thode 3 : VÃ©rifier MongoDB

```bash
# Compter les documents
docker exec -i $(docker ps -q -f name=mongodb) \
  mongosh -u admin -p admin123 --authenticationDatabase admin \
  --quiet --eval \
  "db.getSiblingDB('services_publics').demandes.countDocuments()"

# Ou avec Make
make check-mongo
```

### MÃ©thode 4 : Connexion SQL interactive

```bash
# Se connecter Ã  PostgreSQL
docker exec -it $(docker ps -q -f name=postgres) \
  psql -U airflow -d airflow

# Ou avec Make
make shell-pg
```

Dans psql :
```sql
-- Lister les tables
\dt

-- Voir le schÃ©ma de la table
\d demandes_cleaned

-- Compter les lignes
SELECT COUNT(*) FROM demandes_cleaned;

-- Quitter
\q
```

---

## ğŸ” RequÃªtes SQL utiles

### Statistiques de base

```sql
-- Nombre total de demandes
SELECT COUNT(*) as total_demandes FROM demandes_cleaned;

-- Nombre de demandes par type de service
SELECT type_service, COUNT(*) as nb_demandes
FROM demandes_cleaned
GROUP BY type_service
ORDER BY nb_demandes DESC;

-- Top 10 des communes avec le plus de demandes
SELECT commune, COUNT(*) as nb_demandes
FROM demandes_cleaned
WHERE commune IS NOT NULL
GROUP BY commune
ORDER BY nb_demandes DESC
LIMIT 10;
```

### Analyse par statut

```sql
-- RÃ©partition par statut
SELECT statut, COUNT(*) as nb_demandes,
       ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER (), 2) as pourcentage
FROM demandes_cleaned
GROUP BY statut
ORDER BY nb_demandes DESC;

-- Demandes ouvertes vs fermÃ©es par commune
SELECT commune,
       COUNT(*) as total,
       SUM(CASE WHEN statut IN ('ouverte', 'open', 'pending') THEN 1 ELSE 0 END) as ouvertes,
       SUM(CASE WHEN statut IN ('closed', 'fermÃ©e', 'resolu') THEN 1 ELSE 0 END) as fermees
FROM demandes_cleaned
WHERE commune IS NOT NULL
GROUP BY commune
ORDER BY total DESC
LIMIT 10;
```

### Analyse temporelle

```sql
-- Demandes par annÃ©e
SELECT annee, COUNT(*) as nb_demandes
FROM demandes_cleaned
GROUP BY annee
ORDER BY annee DESC;

-- Demandes par mois (2025)
SELECT annee, mois, COUNT(*) as nb_demandes
FROM demandes_cleaned
WHERE annee = 2025
GROUP BY annee, mois
ORDER BY mois;

-- Demandes par jour de la semaine
SELECT jour_semaine, COUNT(*) as nb_demandes
FROM demandes_cleaned
GROUP BY jour_semaine
ORDER BY nb_demandes DESC;
```

### Tables d'agrÃ©gation

```sql
-- Statistiques par type et localisation
SELECT * FROM stats_type_localisation
ORDER BY nombre_demandes DESC
LIMIT 10;

-- Taux de rÃ©solution par commune
SELECT type_service, commune, nombre_demandes, taux_resolution
FROM stats_type_localisation
WHERE nombre_demandes > 10
ORDER BY taux_resolution DESC
LIMIT 10;

-- Ã‰volution temporelle par type de service
SELECT annee, mois, type_service, nombre_demandes
FROM stats_temporelles
WHERE annee = 2025
ORDER BY annee DESC, mois DESC, nombre_demandes DESC;
```

---

## ğŸ› ï¸ Commandes de gestion

### Avec Make (recommandÃ©)

```bash
# Voir toutes les commandes disponibles
make help

# DÃ©marrer les services
make start

# ArrÃªter les services
make stop

# RedÃ©marrer les services
make restart

# Voir les logs en temps rÃ©el
make logs

# Voir les logs Airflow
make logs-airflow

# VÃ©rifier l'Ã©tat du pipeline
make status

# VÃ©rifier PostgreSQL
make check-pg

# VÃ©rifier MongoDB
make check-mongo

# Charger les donnÃ©es dans MongoDB
make load-data

# DÃ©clencher le DAG manuellement
make trigger-dag

# ExÃ©cuter manuellement l'ingestion Spark
make run-ingestion

# ExÃ©cuter manuellement le processing Spark
make run-processing

# AccÃ©der au shell Airflow
make shell-airflow

# AccÃ©der au shell PostgreSQL
make shell-pg

# AccÃ©der au shell MongoDB
make shell-mongo

# Backup PostgreSQL
make backup-pg

# Backup MongoDB
make backup-mongo

# Nettoyer complÃ¨tement (âš ï¸ supprime les donnÃ©es)
make clean
```

### Sans Make

```bash
# DÃ©marrer
docker-compose up -d

# ArrÃªter
docker-compose down

# Voir les logs
docker-compose logs -f

# Logs d'un service spÃ©cifique
docker-compose logs -f airflow-scheduler

# RedÃ©marrer un service
docker-compose restart airflow-scheduler

# Ã‰tat des conteneurs
docker-compose ps

# Ressources utilisÃ©es
docker stats
```

---

### ProblÃ¨me : MongoDB vide aprÃ¨s installation

```bash
# VÃ©rifier si les donnÃ©es sont chargÃ©es
make check-mongo

# Recharger manuellement
make load-data

# Ou
docker exec -it $(docker ps -q -f name=airflow-scheduler) \
  python /opt/airflow/scripts/load_data_to_mongo.py \
  /opt/data/demandes_services_publics_togo.json
```

### ProblÃ¨me : PostgreSQL vide

```bash
# VÃ©rifier les logs du DAG
docker-compose logs airflow-scheduler | grep ERROR

# Relancer le processing manuellement
make run-processing
```

### ProblÃ¨me : Airflow ne dÃ©marre pas

```bash
# VÃ©rifier les logs
docker-compose logs airflow-init
docker-compose logs airflow-webserver

# RÃ©initialiser complÃ¨tement
docker-compose down -v
docker-compose up -d
```

### ProblÃ¨me : Job Spark Ã©choue

```bash
# Voir les logs dÃ©taillÃ©s
docker-compose logs airflow-scheduler | grep -A 50 "ingestion_mongodb"

# VÃ©rifier les ressources
docker stats

# Augmenter la mÃ©moire Docker (Settings â†’ Resources)
```

### ProblÃ¨me : Erreur "Cannot connect to Docker daemon"

```bash
# DÃ©marrer Docker Desktop
# Attendre qu'il soit complÃ¨tement lancÃ© (icÃ´ne verte)

# VÃ©rifier
docker ps
```

### ProblÃ¨me : Port 8080 dÃ©jÃ  utilisÃ©

```bash
# Changer le port dans docker-compose.yml
# Ligne : "8080:8080" â†’ "8081:8080"

# RedÃ©marrer
docker-compose down
docker-compose up -d

# AccÃ©der via http://localhost:8081
```

### Logs utiles pour diagnostic

```bash
# Tous les logs
docker-compose logs --tail=100

# Logs Airflow Scheduler
docker-compose logs --tail=50 airflow-scheduler

# Logs d'une tÃ¢che spÃ©cifique
docker exec -it $(docker ps -q -f name=airflow-scheduler) \
  tail -100 /opt/airflow/logs/dag_id=pipeline_services_publics_togo/*/task_id=ingestion_mongodb/attempt=1.log
```

---

## ğŸ“ Structure du projet

```
pipeline-services-publics/
â”‚
â”œâ”€â”€ README.md                           # Cette documentation
â”œâ”€â”€ docker-compose.yml                  # Configuration Docker Compose
â”œâ”€â”€ Dockerfile                          # Image Airflow + Spark
â”œâ”€â”€ requirements.txt                    # DÃ©pendances Python
â”‚
â”œâ”€â”€ dags/                               # DAGs Airflow
â”‚   â””â”€â”€ pipeline_services_publics.py    # DAG principal (7 tÃ¢ches)
â”‚
â”œâ”€â”€ spark_jobs/                         # Jobs Spark PySpark
â”‚   â”œâ”€â”€ ingestion.py                    # MongoDB â†’ Parquet (RAW)
â”‚   â””â”€â”€ processing.py                   # Nettoyage â†’ PostgreSQL
â”‚
â”œâ”€â”€ scripts/                            # Scripts utilitaires
â”‚   â””â”€â”€ load_data_to_mongo.py          # Chargement initial dans MongoDB
â”‚
â”œâ”€â”€ init-mongo/                         # Scripts d'initialisation MongoDB
â”‚   â””â”€â”€ init.js                         # CrÃ©ation de la base et index
â”‚
â”œâ”€â”€ init-postgres/                      # Scripts d'initialisation